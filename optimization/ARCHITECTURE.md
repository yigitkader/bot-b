# Optimization Architecture

## ðŸ—ï¸ Mimari Prensipler

### 1. Separation of Concerns
- **Python Layer**: High-level optimization coordination, config management, result analysis
- **Rust Layer**: Backtest execution, performance-critical calculations
- **Tests Layer**: Unit and integration tests only

### 2. Single Source of Truth
- All optimization logic in `optimization/` package
- Config-driven parameter management
- Centralized result storage

### 3. Extensibility
- Plugin-based parameter space definitions
- Strategy pattern for different optimization algorithms
- Easy to add new metrics and analyzers

## ðŸ“ Dizin YapÄ±sÄ±

```
optimization/
â”œâ”€â”€ __init__.py              # Package exports
â”œâ”€â”€ __main__.py              # CLI entry point
â”œâ”€â”€ parameter_space.py        # Parameter space definitions
â”œâ”€â”€ config_manager.py         # Config.yaml management
â”œâ”€â”€ backtest_runner.py        # Backtest execution & parsing
â”œâ”€â”€ result_analyzer.py       # Result analysis & reporting
â”œâ”€â”€ optimizer.py              # Main optimization coordinator
â”œâ”€â”€ cli.py                    # Command-line interface
â”œâ”€â”€ strategies/               # Optimization strategies (future)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ grid_search.py        # Grid search strategy
â”‚   â”œâ”€â”€ random_search.py      # Random search strategy
â”‚   â””â”€â”€ bayesian.py           # Bayesian optimization (future)
â”œâ”€â”€ metrics/                  # Custom metrics (future)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sharpe.py
â”‚   â””â”€â”€ drawdown.py
â””â”€â”€ README.md                 # Documentation
```

## ðŸ”„ Data Flow

```
User Input (CLI)
    â†“
ParameterSpace (defines search space)
    â†“
Optimizer (coordinates)
    â†“
ConfigManager (applies params to config.yaml)
    â†“
BacktestRunner (executes cargo test)
    â†“
ResultAnalyzer (parses & analyzes)
    â†“
Results (JSON output)
```

## ðŸŽ¯ GeniÅŸletilebilirlik

### Yeni Parametre Ekleme

1. **ParameterSpace'e ekle:**
```python
class ParameterSpace:
    def __init__(self, ..., new_param: List[float] = None):
        self.new_param = new_param or [default_values]
    
    def generate_combinations(self):
        for new_val in self.new_param:
            # Add to combinations
```

2. **ConfigManager'e ekle:**
```python
def apply_parameters(self, params: ParameterSet):
    # Add new parameter update logic
    if line.strip().startswith('new_param:'):
        lines[i] = f"{indent}new_param: {params.new_param}\n"
```

3. **ParameterSet'e ekle:**
```python
@dataclass
class ParameterSet:
    # ... existing fields
    new_param: float
```

### Yeni Optimizasyon Stratejisi

1. **Strategy interface oluÅŸtur:**
```python
# optimization/strategies/base.py
class OptimizationStrategy:
    def search(self, space: ParameterSpace) -> Iterator[ParameterSet]:
        raise NotImplementedError
```

2. **Yeni strateji implement et:**
```python
# optimization/strategies/random_search.py
class RandomSearchStrategy(OptimizationStrategy):
    def search(self, space: ParameterSpace) -> Iterator[ParameterSet]:
        # Random sampling logic
```

3. **Optimizer'a entegre et:**
```python
optimizer = Optimizer(
    parameter_space=space,
    strategy=RandomSearchStrategy()
)
```

### Yeni Metrik Ekleme

1. **BacktestRunner'da parse et:**
```python
def _parse_output(self, output: str) -> BacktestResult:
    # Add new metric parsing
    new_metric_match = re.search(r'New Metric: ([\d.]+)', output)
    new_metric = float(new_metric_match.group(1)) if new_metric_match else 0.0
```

2. **BacktestResult'a ekle:**
```python
@dataclass
class BacktestResult:
    # ... existing fields
    new_metric: float
```

3. **ResultAnalyzer'da kullan:**
```python
def get_best_by_new_metric(self) -> Optional[Dict]:
    return max(self.results, key=lambda x: x['new_metric'])
```

## ðŸ§ª Test Stratejisi

### Unit Tests
- `tests/optimization/` - Python optimization package tests
- Test each module independently

### Integration Tests
- `tests/backtest.rs` - Backtest execution tests
- Verify end-to-end optimization flow

### E2E Tests
- Full optimization run with small parameter space
- Verify results are saved correctly

## ðŸ“Š Monitoring & Observability

### Progress Tracking
- Real-time progress updates
- Estimated time remaining
- Current best result

### Result Persistence
- JSON results file
- Config backups
- Optimization history

### Error Handling
- Graceful degradation
- Error recovery
- Detailed error logging

## ðŸš€ Future Enhancements

1. **Parallel Execution**: Run multiple backtests concurrently
2. **Early Stopping**: Stop if no improvement after N iterations
3. **Adaptive Search**: Focus on promising regions
4. **ML Integration**: Use ML to predict good parameters
5. **Visualization**: Plot optimization progress and results

